# Reinforcement-learning-for-dynamic-systems-application-of-Fitted-Q-Iteration-methods-
This thesis explores the theoretical foundations of RL with a focus on offline and off-policy methods, analyzing in particular the Fitted Q-Iteration (FQI) algorithm. A further contribution of this work is the integration of Conformal Prediction (CP) to enhance the reliability of policy evaluation. 

# Introduction
In recent decades, the idea has become increasingly consolidated that machines can exploit available information to solve problems and make decisions, drawing inspiration from the mechanisms of human learning. Today, the science of learning plays a central role in statistics, machine learning, and artificial intelligence, with cross-disciplinary applications in fields such as medicine, finance, and engineering. Humans naturally learn by interacting with their environment: from these interactions arises a wealth of information about causes, effects, and consequences of actions, enabling adaptation of behavior to achieve specific goals. Following this model, several methods have been developed that allow machines to tackle complex decision-making problems in dynamic environments.

Within this context, Reinforcement Learning (RL) stands out as a particularly effective approach. Here, a learning agent—such as software, a robot, or an autonomous vehicle—interacts with its environment, receiving rewards in the form of numerical values that depend on the executed actions. Through iterative learning, the agent progressively improves its decisions with the aim of maximizing the cumulative reward \citep{sutton2018reinforcement}. RL has its roots in behavioral psychology, where it was shown that animals can be taught to perform even complex tasks using simple mechanisms, such as rewarding them after the required action is completed. The underlying idea is that an agent—human, animal, or artificial—can learn to optimize its behavior based on past experiences.

Among the various RL algorithms in the literature, Q-learning is one of the most well-known. However, its direct application is limited when the state and action spaces are large or continuous, or when direct interaction with the environment is not feasible. In such cases, the Fitted Q-Iteration (FQI) method is employed, an offline approach in which the agent learns from a pre-existing dataset rather than through direct interaction. Based on the available data, the agent determines the decision rule (policy) to be followed at each time step, approximating the optimal one. Specifically, the agent estimates the action-value function 
Q, which measures the quality of being in a given state and following a certain policy. FQI uses supervised learning models to approximate the Q-function and derive optimal policies from the dataset (ernst2005tree), making it suitable for sequential decision-making problems.

Traditional predictive models focus on point forecasts, which often lack information about reliability. In real-world problems, quantifying uncertainty is crucial for safe and reliable decisions. In applications such as dynamic system control, it is not enough to obtain a point estimate: it is also necessary to know the confidence level of predictions to support decision-making. To address this, the Conformal Prediction (CP) technique was developed, offering a simple and intuitive framework for constructing prediction intervals that, by definition, contain the true value of the variable of interest with high probability (angelopoulos2021gentle). CP is a distribution-free method capable of quantifying uncertainty in predictions generated by arbitrary algorithms, requiring only the assumption of data exchangeability. However, this assumption may be violated in practice due to distribution shift between training and test data. In such cases, classical CP may produce distorted intervals. To overcome this, Weighted Conformal Prediction was proposed (tibshirani2019conformal). In the context of sequential decision-making, the objective of CP is to construct prediction intervals for the Q-value estimates produced by FQI. To this end, a tailored algorithm was proposed, extending the Conformal Off-Policy Prediction (COPP) approach of (zhang2023conformal).

In offline RL settings, datasets typically originate from the execution of a dynamic system under a given policy. This thesis focuses on the study and application of FQI with different regression models, using the House Heating dataset from (candelieri2023safe). The task is to control the indoor temperature of a building over 60 days, selecting actions that ensure thermal comfort while minimizing operating costs.

This thesis proposes the integration of FQI with CP methods. The main objective is to assess the effectiveness of the FQI-based approach for dynamic system control, highlighting both its strengths and limitations. In particular, different regression models are compared within FQI, identifying the one that yields the highest cumulative reward. A secondary goal is to analyze the contribution of CP for sequential algorithms such as FQI, based on empirical results, thereby laying the groundwork for future methodological developments in uncertainty quantification for reinforcement learning.

The first chapter introduces the fundamentals of RL, focusing on Markov Decision Processes.
The second chapter discusses key RL models, with emphasis on Q-learning and its off-policy and offline variants, followed by a detailed presentation of FQI and the regression models employed.
The third chapter explores CP, beginning with its classical formulation and then specializing it to off-policy algorithms, particularly FQI.
Finally, the fourth chapter applies these methods to the House Heating dataset: first using FQI to identify optimal temperature control actions, and then employing CP to quantify the uncertainty of predictions generated by FQI.
